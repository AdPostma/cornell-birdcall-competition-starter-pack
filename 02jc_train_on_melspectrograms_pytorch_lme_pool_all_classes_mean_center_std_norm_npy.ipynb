{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "from birdcall.metrics import *\n",
    "from birdcall.ops import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16\n",
    "MAX_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_pickle('data/classes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_items = pd.read_pickle('data/all_train_items.pkl')\n",
    "\n",
    "# all_train_items_npy = []\n",
    "\n",
    "# for ebird_code, path, duration in all_train_items:\n",
    "#     fn = path.stem\n",
    "#     new_path = Path(f'data/npy/train_resampled/{ebird_code}/{fn}.npy')\n",
    "#     all_train_items_npy.append((ebird_code, new_path, duration))\n",
    "    \n",
    "# pd.to_pickle(all_train_items_npy, 'data/all_train_items_npy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pd.read_pickle('data/all_splits.pkl')\n",
    "all_train_items = pd.read_pickle('data/all_train_items_npy.pkl')\n",
    "\n",
    "train_items = np.array(all_train_items)[splits[0][0]].tolist()\n",
    "val_items = np.array(all_train_items)[splits[0][1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# class2train_items = defaultdict(list)\n",
    "\n",
    "# for cls_name, path, duration in train_items:\n",
    "#     class2train_items[cls_name].append((path, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(class2train_items, 'data/class2train_items.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2train_items = pd.read_pickle('data/class2train_items.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MelspecPoolDataset(class2train_items, classes, len_mult=50, normalize=False)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BS, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = [(classes.index(item[0]), item[1], item[2]) for item in val_items]\n",
    "val_items_binned = bin_items_negative_class(val_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(*list(torchvision.models.resnet34(True).children())[:-2])\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, len(classes))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.log10(1 + x)\n",
    "        mins = x.view(x.shape[0], x.shape[1], -1).min(-1)[0]\n",
    "        maxs = x.view(x.shape[0], x.shape[1], -1).max(-1)[0]\n",
    "\n",
    "        good_ones = (maxs - mins) >  1e-4\n",
    "\n",
    "        x[good_ones] -= x[good_ones].mean((1,2,3))[:, None, None, None]\n",
    "        x[good_ones] -= x[good_ones].std((1,2,3))[:, None, None, None]\n",
    "\n",
    "        x[~good_ones] = x[~good_ones].zero_()\n",
    "        \n",
    "        bs, im_num = x.shape[:2]\n",
    "        x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean((2,3))\n",
    "        x = self.classifier(x)\n",
    "        x = x.view(bs, im_num, -1)\n",
    "        x = lme_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 3, 80, 212])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(batch[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), MAX_LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_items = pd.read_pickle('data/soundscape_items.pkl')\n",
    "\n",
    "# sc_items_npy = []\n",
    "# for labels, path, offset in sc_items:\n",
    "#     sc_items_npy.append((labels, Path(f'data/npy/shifted/{path.stem}.npy'), offset))\n",
    "    \n",
    "# pd.to_pickle(sc_items_npy, 'data/soundscape_items_npy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ds = SoundscapeMelspecPoolDataset(pd.read_pickle('data/soundscape_items_npy.pkl'), classes)\n",
    "sc_dl = torch.utils.data.DataLoader(sc_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 16.0] loss: 0.023, f1: 0.001, sc_f1: 0.000\n",
      "[10, 31.3] loss: 0.020, f1: 0.008, sc_f1: 0.000\n",
      "[15, 46.6] loss: 0.018, f1: 0.085, sc_f1: 0.000\n",
      "[20, 62.1] loss: 0.015, f1: 0.227, sc_f1: 0.000\n",
      "[25, 77.4] loss: 0.013, f1: 0.353, sc_f1: 0.000\n",
      "[30, 92.7] loss: 0.012, f1: 0.476, sc_f1: 0.000\n",
      "[35, 108.0] loss: 0.010, f1: 0.554, sc_f1: 0.000\n",
      "[40, 123.3] loss: 0.008, f1: 0.593, sc_f1: 0.000\n",
      "[45, 138.6] loss: 0.007, f1: 0.629, sc_f1: 0.000\n",
      "[50, 153.8] loss: 0.007, f1: 0.645, sc_f1: 0.000\n",
      "[55, 169.1] loss: 0.006, f1: 0.645, sc_f1: 0.000\n",
      "[60, 184.5] loss: 0.005, f1: 0.683, sc_f1: 0.000\n",
      "[65, 199.7] loss: 0.005, f1: 0.682, sc_f1: 0.000\n",
      "[70, 215.1] loss: 0.004, f1: 0.692, sc_f1: 0.000\n",
      "[75, 230.4] loss: 0.004, f1: 0.696, sc_f1: 0.000\n",
      "[80, 245.7] loss: 0.004, f1: 0.688, sc_f1: 0.000\n",
      "[85, 261.0] loss: 0.003, f1: 0.700, sc_f1: 0.000\n",
      "[90, 276.3] loss: 0.003, f1: 0.703, sc_f1: 0.022\n",
      "[95, 291.6] loss: 0.003, f1: 0.708, sc_f1: 0.000\n",
      "[100, 306.8] loss: 0.003, f1: 0.705, sc_f1: 0.000\n",
      "[105, 322.1] loss: 0.003, f1: 0.702, sc_f1: 0.000\n",
      "[110, 337.4] loss: 0.003, f1: 0.712, sc_f1: 0.000\n",
      "[115, 352.7] loss: 0.003, f1: 0.700, sc_f1: 0.000\n",
      "[120, 368.0] loss: 0.002, f1: 0.712, sc_f1: 0.000\n",
      "[125, 383.3] loss: 0.002, f1: 0.706, sc_f1: 0.000\n",
      "[130, 398.6] loss: 0.002, f1: 0.712, sc_f1: 0.000\n",
      "[135, 413.9] loss: 0.002, f1: 0.708, sc_f1: 0.000\n",
      "[140, 429.1] loss: 0.002, f1: 0.708, sc_f1: 0.000\n",
      "[145, 444.4] loss: 0.002, f1: 0.709, sc_f1: 0.000\n",
      "[150, 459.7] loss: 0.002, f1: 0.717, sc_f1: 0.000\n",
      "[155, 475.0] loss: 0.002, f1: 0.721, sc_f1: 0.000\n",
      "[160, 490.3] loss: 0.002, f1: 0.718, sc_f1: 0.000\n",
      "[165, 505.6] loss: 0.002, f1: 0.708, sc_f1: 0.000\n",
      "[170, 520.9] loss: 0.001, f1: 0.723, sc_f1: 0.000\n",
      "[175, 536.2] loss: 0.001, f1: 0.719, sc_f1: 0.000\n",
      "[180, 551.5] loss: 0.001, f1: 0.716, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(180):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if np.isnan(loss.item()): \n",
    "            raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_log_zero_center_std_norm_{round(max(f1s), 2)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/180_lmepool_simple_log_zero_center_std_norm_0.72.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(180, 360):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if np.isnan(loss.item()): \n",
    "            raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_log_zero_center_std_norm_{round(max(f1s), 2)}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
