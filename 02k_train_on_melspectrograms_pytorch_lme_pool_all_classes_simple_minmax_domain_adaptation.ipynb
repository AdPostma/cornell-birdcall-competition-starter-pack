{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1409.7495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "from birdcall.metrics import *\n",
    "from birdcall.ops import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_pickle('data/classes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pd.read_pickle('data/all_splits.pkl')\n",
    "all_train_items = pd.read_pickle('data/all_train_items.pkl')\n",
    "\n",
    "train_items = np.array(all_train_items)[splits[0][0]].tolist()\n",
    "val_items = np.array(all_train_items)[splits[0][1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2train_items = defaultdict(list)\n",
    "\n",
    "for cls_name, path, duration in train_items:\n",
    "    class2train_items[cls_name].append((path, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MelspecPoolWithShiftedDataset(class2train_items, classes, len_mult=50)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = [(classes.index(item[0]), item[1], item[2]) for item in val_items]\n",
    "val_items_binned = bin_items_negative_class(val_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(p, delta=10):\n",
    "    return 2 / (1 + np.exp(-delta*p)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return - alpha(p) * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, p=0.5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=p), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=p), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_classes)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(*list(torchvision.models.resnet34(True).children())[:-2])\n",
    "        self.ebird_classifier = Classifier(len(classes))\n",
    "        self.domain_classifier = Classifier(1)\n",
    "        self.grad_reversal = GradientReversal()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.log10(1 + x)\n",
    "        max_per_example = x.view(x.shape[0], -1).max(1)[0] # scaling to between 0 and 1\n",
    "        x /= max_per_example[:, None, None, None, None]     # per example!\n",
    "        bs, im_num = x.shape[:2]\n",
    "        x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = self.cnn(x)\n",
    "        per_image_features = x.mean((2,3))\n",
    "        \n",
    "        ebird_logits = self.ebird_classifier(per_image_features)\n",
    "        \n",
    "        reversed_grads = self.grad_reversal.apply(per_image_features)\n",
    "        domain_logits = self.domain_classifier(reversed_grads)\n",
    "        \n",
    "        ebird_preds = lme_pool(ebird_logits.view(bs, im_num, -1))\n",
    "        domain_preds = lme_pool(domain_logits.view(bs, im_num, -1))\n",
    "        \n",
    "        return ebird_preds, domain_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('models/235_lmepool_simple_minmax_log_0.74.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['ebird_classifier.layers.0.weight', 'ebird_classifier.layers.0.bias', 'ebird_classifier.layers.3.weight', 'ebird_classifier.layers.3.bias', 'ebird_classifier.layers.3.running_mean', 'ebird_classifier.layers.3.running_var', 'ebird_classifier.layers.4.weight', 'ebird_classifier.layers.4.bias', 'ebird_classifier.layers.7.weight', 'ebird_classifier.layers.7.bias', 'ebird_classifier.layers.7.running_mean', 'ebird_classifier.layers.7.running_var', 'ebird_classifier.layers.8.weight', 'ebird_classifier.layers.8.bias', 'domain_classifier.layers.0.weight', 'domain_classifier.layers.0.bias', 'domain_classifier.layers.3.weight', 'domain_classifier.layers.3.bias', 'domain_classifier.layers.3.running_mean', 'domain_classifier.layers.3.running_var', 'domain_classifier.layers.4.weight', 'domain_classifier.layers.4.bias', 'domain_classifier.layers.7.weight', 'domain_classifier.layers.7.bias', 'domain_classifier.layers.7.running_mean', 'domain_classifier.layers.7.running_var', 'domain_classifier.layers.8.weight', 'domain_classifier.layers.8.bias'], unexpected_keys=['classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.3.running_mean', 'classifier.3.running_var', 'classifier.3.num_batches_tracked', 'classifier.4.weight', 'classifier.4.bias', 'classifier.7.weight', 'classifier.7.bias', 'classifier.7.running_mean', 'classifier.7.running_var', 'classifier.7.num_batches_tracked', 'classifier.8.weight', 'classifier.8.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ds = SoundscapeMelspecPoolDataset(pd.read_pickle('data/soundscape_items.pkl'), classes)\n",
    "sc_dl = torch.utils.data.DataLoader(sc_ds, batch_size=2*16, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 19.1] loss: 0.504, f1: 0.247, sc_f1: 0.000\n",
      "[10, 41.8] loss: 0.503, f1: 0.570, sc_f1: 0.000\n",
      "[15, 63.5] loss: 0.508, f1: 0.607, sc_f1: 0.000\n",
      "[20, 86.6] loss: 0.509, f1: 0.630, sc_f1: 0.010\n",
      "[25, 109.3] loss: 0.513, f1: 0.589, sc_f1: 0.000\n",
      "[30, 133.6] loss: 0.522, f1: 0.648, sc_f1: 0.000\n",
      "[35, 159.0] loss: 0.517, f1: 0.655, sc_f1: 0.000\n",
      "[40, 182.4] loss: 0.516, f1: 0.573, sc_f1: 0.000\n",
      "[45, 203.5] loss: 0.520, f1: 0.618, sc_f1: 0.000\n",
      "[50, 228.6] loss: 0.507, f1: 0.675, sc_f1: 0.011\n",
      "[55, 252.5] loss: 0.508, f1: 0.584, sc_f1: 0.000\n",
      "[60, 274.6] loss: 0.511, f1: 0.652, sc_f1: 0.000\n",
      "[65, 299.9] loss: 0.515, f1: 0.622, sc_f1: 0.000\n",
      "[70, 322.2] loss: 0.508, f1: 0.577, sc_f1: 0.000\n",
      "[75, 344.5] loss: 0.512, f1: 0.595, sc_f1: 0.000\n",
      "[80, 368.1] loss: 0.508, f1: 0.393, sc_f1: 0.000\n",
      "[85, 390.6] loss: 0.513, f1: 0.366, sc_f1: 0.000\n",
      "[90, 413.6] loss: 0.515, f1: 0.321, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "total_epochs = 260\n",
    "for epoch in range(total_epochs):\n",
    "    running_loss = 0.0\n",
    "    p = (epoch + 1) / total_epochs\n",
    "    for data in train_dl:\n",
    "        model.train()\n",
    "        inputs, ebird_labels, domain_labels = data[0].cuda(), data[1].cuda(), data[2].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ebird_preds, domain_preds = model(inputs)\n",
    "        ebird_loss = criterion(ebird_preds[domain_labels == 0], ebird_labels[domain_labels == 0])\n",
    "        domain_loss = criterion(domain_preds, domain_labels.unsqueeze(1))\n",
    "        loss = ebird_loss + domain_loss\n",
    "\n",
    "        if np.isnan(loss.item()): raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*16, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)[0]\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)[0]\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_minmax_log_da_{round(max(f1s), 2)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 21.9] loss: 0.514, f1: 0.485, sc_f1: 0.000\n",
      "[10, 44.2] loss: 0.512, f1: 0.537, sc_f1: 0.000\n",
      "[15, 66.4] loss: 0.520, f1: 0.576, sc_f1: 0.000\n",
      "[20, 87.7] loss: 0.509, f1: 0.538, sc_f1: 0.000\n",
      "[25, 110.8] loss: 0.518, f1: 0.509, sc_f1: 0.000\n",
      "[30, 132.6] loss: 0.506, f1: 0.539, sc_f1: 0.000\n",
      "[35, 153.4] loss: 0.511, f1: 0.423, sc_f1: 0.000\n",
      "[40, 176.2] loss: 0.510, f1: 0.534, sc_f1: 0.000\n",
      "[45, 197.9] loss: 0.517, f1: 0.583, sc_f1: 0.000\n",
      "[50, 219.3] loss: 0.505, f1: 0.512, sc_f1: 0.000\n",
      "[55, 240.7] loss: 0.521, f1: 0.522, sc_f1: 0.000\n",
      "[60, 262.7] loss: 0.511, f1: 0.542, sc_f1: 0.000\n",
      "[65, 285.0] loss: 0.512, f1: 0.542, sc_f1: 0.000\n",
      "[70, 307.3] loss: 0.520, f1: 0.531, sc_f1: 0.000\n",
      "[75, 329.1] loss: 0.516, f1: 0.500, sc_f1: 0.000\n",
      "[80, 350.1] loss: 0.518, f1: 0.453, sc_f1: 0.000\n",
      "[85, 371.7] loss: 0.516, f1: 0.470, sc_f1: 0.000\n",
      "[90, 393.3] loss: 0.513, f1: 0.488, sc_f1: 0.000\n",
      "[95, 415.2] loss: 0.518, f1: 0.456, sc_f1: 0.000\n",
      "[100, 437.2] loss: 0.509, f1: 0.453, sc_f1: 0.000\n",
      "[105, 459.7] loss: 0.509, f1: 0.544, sc_f1: 0.000\n",
      "[110, 482.0] loss: 0.517, f1: 0.551, sc_f1: 0.000\n",
      "[115, 503.9] loss: 0.521, f1: 0.473, sc_f1: 0.000\n",
      "[120, 525.5] loss: 0.520, f1: 0.502, sc_f1: 0.000\n",
      "[125, 546.9] loss: 0.515, f1: 0.452, sc_f1: 0.000\n",
      "[130, 569.3] loss: 0.509, f1: 0.536, sc_f1: 0.000\n",
      "[135, 592.4] loss: 0.512, f1: 0.415, sc_f1: 0.000\n",
      "[140, 613.6] loss: 0.505, f1: 0.488, sc_f1: 0.000\n",
      "[145, 634.6] loss: 0.511, f1: 0.484, sc_f1: 0.000\n",
      "[150, 656.6] loss: 0.517, f1: 0.491, sc_f1: 0.000\n",
      "[155, 678.2] loss: 0.527, f1: 0.522, sc_f1: 0.000\n",
      "[160, 700.0] loss: 0.516, f1: 0.497, sc_f1: 0.000\n",
      "[165, 722.6] loss: 0.522, f1: 0.538, sc_f1: 0.000\n",
      "[170, 745.0] loss: 0.516, f1: 0.549, sc_f1: 0.000\n",
      "[175, 768.3] loss: 0.518, f1: 0.544, sc_f1: 0.006\n",
      "[180, 792.6] loss: 0.506, f1: 0.585, sc_f1: 0.000\n",
      "[185, 818.0] loss: 0.512, f1: 0.523, sc_f1: 0.000\n",
      "[190, 841.7] loss: 0.514, f1: 0.567, sc_f1: 0.000\n",
      "[195, 872.9] loss: 0.513, f1: 0.540, sc_f1: 0.000\n",
      "[200, 902.7] loss: 0.508, f1: 0.594, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "total_epochs = 200\n",
    "for epoch in range(total_epochs):\n",
    "    running_loss = 0.0\n",
    "    p = (epoch + 1) / total_epochs\n",
    "    for data in train_dl:\n",
    "        model.train()\n",
    "        inputs, ebird_labels, domain_labels = data[0].cuda(), data[1].cuda(), data[2].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ebird_preds, domain_preds = model(inputs)\n",
    "        ebird_loss = criterion(ebird_preds[domain_labels == 0], ebird_labels[domain_labels == 0])\n",
    "        domain_loss = criterion(domain_preds, domain_labels.unsqueeze(1))\n",
    "        loss = ebird_loss + domain_loss\n",
    "\n",
    "        if np.isnan(loss.item()): raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*16, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)[0]\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)[0]\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_minmax_log_da_{round(max(f1s), 2)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models/200_lmepool_simple_minmax_log_da_0.59.pth' target='_blank'>models/200_lmepool_simple_minmax_log_da_0.59.pth</a><br>"
      ],
      "text/plain": [
       "/home/radek/workspace/birdcall/models/200_lmepool_simple_minmax_log_da_0.59.pth"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('models/200_lmepool_simple_minmax_log_da_0.59.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
