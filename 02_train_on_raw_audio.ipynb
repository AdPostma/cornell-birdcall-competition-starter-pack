{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us get all the data that we need. Through the magic of `nbdev`, we will use the functionality we defined in `01_gettin_started`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "\n",
    "items, classes = get_items(1000)\n",
    "trn_idxs, val_idxs = trn_val_split_items(items, 10)[0]\n",
    "mean, std = calculate_mean_and_std(items, trn_idxs)\n",
    "trn_ds = AudioDataset(items[trn_idxs], classes, mean, std)\n",
    "val_ds = AudioDataset(items[val_idxs], classes, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237600, 26400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some sort of architecture to get started - the one adapted from this [paper](https://www.groundai.com/project/end-to-end-environmental-sound-classification-using-a-1d-convolutional-neural-network/1) seems like a good place to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128\n",
    "\n",
    "dls = DataLoaders(\n",
    "    DataLoader(dataset=trn_ds, bs=BS, num_workers=NUM_WORKERS, shuffle=True),\n",
    "    DataLoader(dataset=val_ds, bs=BS, num_workers=NUM_WORKERS)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 160000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.train.one_batch()\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_arch = lambda: nn.Sequential(*[\n",
    "    Lambda(lambda x: x.unsqueeze(1)),\n",
    "    ConvLayer(1, 16, ks=64, stride=2, ndim=1),\n",
    "    ConvLayer(16, 16, ks=8, stride=8, ndim=1),\n",
    "    ConvLayer(16, 32, ks=32, stride=2, ndim=1),\n",
    "    ConvLayer(32, 32, ks=8, stride=8, ndim=1),\n",
    "    ConvLayer(32, 64, ks=16, stride=2, ndim=1),\n",
    "    ConvLayer(64, 128, ks=8, stride=2, ndim=1),\n",
    "    ConvLayer(128, 256, ks=4, stride=2, ndim=1),\n",
    "    ConvLayer(256, 256, ks=4, stride=4, ndim=1),\n",
    "    Flatten(),\n",
    "    LinBnDrop(5120, 512, p=0.25, act=nn.ReLU()),\n",
    "    LinBnDrop(512, 512, p=0.25, act=nn.ReLU()),\n",
    "    LinBnDrop(512, 256, p=0.25, act=nn.ReLU()),\n",
    "    LinBnDrop(256, len(classes)),\n",
    "    nn.Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of functions to help us calculate metrics for diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def preds_to_tp_fp_fn(preds, targs):\n",
    "    positives = preds > 0.5\n",
    "    true_positives = positives[targs == 1]\n",
    "    false_positives = positives[targs != 1]\n",
    "    negatives = ~positives\n",
    "    false_negatives = negatives[targs == 1]\n",
    "    return true_positives.sum(), false_positives.sum(), false_negatives.sum()\n",
    "\n",
    "def precision(preds, targs):\n",
    "    tp, fp, fn = preds_to_tp_fp_fn(preds, targs)\n",
    "    return (tp.float() / (tp + fp)).item()\n",
    "\n",
    "def recall(preds, targs):\n",
    "    tp, fp, fn = preds_to_tp_fp_fn(preds, targs)\n",
    "    return (tp.float() / (tp + fn)).item()\n",
    "\n",
    "def f1(preds, targs, eps=1e-8):\n",
    "    prec = precision(preds, targs)\n",
    "    rec = recall(preds, targs)\n",
    "    return 2 * (prec * rec) / (prec + rec + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    get_arch(),\n",
    "    metrics=[AccumMetric(precision), AccumMetric(recall), AccumMetric(f1)],\n",
    "    loss_func=BCELossFlat()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>03:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops! This is not a good sign. How come our model is that good? Do we have a bug in how we sample the validation set? Was it a bad idea after all to combine all the files together? Given how I have set this up, we could have the first 5 seconds of a recording go into the train set and the next five into the validation set. This doesn't make a lot of sense indeed - we want our model to be able to identify the same species across recordings with different backgrounds / recorded with different equipment - we should be building train and validation sets based off different files.\n",
    "\n",
    "Maybe our metrics have a bug or there is some issue with our model / how the loss gets applied?\n",
    "\n",
    "My money is on the issue with sampling the validation set. But nonetheless, this will not stop us! The first order of business is to create an end to end pipeline, all the way to successful submission. Once we have this in place, we will be in a good position to start fiddling with making improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir data/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model.state_dict(), 'data/models/first_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following info for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.132126808166504e-05, 0.04304003225515465)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(classes, 'data/classes.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
