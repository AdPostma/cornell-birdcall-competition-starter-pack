---

title: Title

keywords: fastai
sidebar: home_sidebar



---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02c_train_on_melspectrograms_pytorch.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">birdcall.data</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">birdcall.metrics</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;data/classes.pkl&#39;</span><span class="p">)</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">MelspecPoolDataset</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;data/train_set.pkl&#39;</span><span class="p">),</span> <span class="n">classes</span><span class="p">,</span> <span class="n">len_mult</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">MelspecPoolDataset</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;data/val_set.pkl&#39;</span><span class="p">),</span> <span class="n">classes</span><span class="p">,</span> <span class="n">len_mult</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(7920, 1320)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span> <span class="k">break</span>
<span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([16, 10, 3, 80, 212]),
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(0.2136), tensor(1.6780))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet34</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">())[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
        <span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">bs</span><span class="p">,</span> <span class="n">im_num</span><span class="p">,</span> <span class="n">ch</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">,</span> <span class="n">x_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ch</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">im_num</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1e-3</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-4-b39cd1ba1307&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> 
<span class="ansi-green-intense-fg ansi-bold">      3</span> criterion <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>BCEWithLogitsLoss<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> </span>optimizer <span class="ansi-blue-fg">=</span> optim<span class="ansi-blue-fg">.</span>Adam<span class="ansi-blue-fg">(</span>model<span class="ansi-blue-fg">.</span>parameters<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1e-3</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> scheduler <span class="ansi-blue-fg">=</span> optim<span class="ansi-blue-fg">.</span>lr_scheduler<span class="ansi-blue-fg">.</span>CosineAnnealingLR<span class="ansi-blue-fg">(</span>optimizer<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;model&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">330</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">targs</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">:</span>
                    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                    <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                    <span class="n">targs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

                <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
                <span class="n">targs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">targs</span><span class="p">)</span>
            
            <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">targs</span><span class="p">)</span>
            <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">targs</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, acc: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, f1: </span><span class="si">{</span><span class="n">f1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span> <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="s1">&#39;models/</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1, 143.8] loss: 0.146, acc: 0.000, f1: 0.005
[2, 139.4] loss: 0.025, acc: 0.000, f1: 0.015
[3, 139.4] loss: 0.025, acc: 0.000, f1: 0.004
[4, 140.3] loss: 0.025, acc: 0.000, f1: 0.006
[5, 138.3] loss: 0.025, acc: 0.000, f1: 0.024
[6, 140.3] loss: 0.024, acc: 0.000, f1: 0.010
[7, 139.1] loss: 0.023, acc: 0.000, f1: 0.022
[8, 140.5] loss: 0.023, acc: 0.002, f1: 0.017
[9, 139.7] loss: 0.022, acc: 0.000, f1: 0.007
[10, 138.9] loss: 0.022, acc: 0.000, f1: 0.021
[11, 139.2] loss: 0.021, acc: 0.001, f1: 0.024
[12, 138.7] loss: 0.021, acc: 0.000, f1: 0.008
[13, 140.9] loss: 0.021, acc: 0.004, f1: 0.026
[14, 140.4] loss: 0.020, acc: 0.005, f1: 0.025
[15, 138.7] loss: 0.020, acc: 0.008, f1: 0.029
[16, 140.1] loss: 0.020, acc: 0.008, f1: 0.029
[17, 140.3] loss: 0.019, acc: 0.011, f1: 0.045
[19, 140.1] loss: 0.018, acc: 0.036, f1: 0.118
[20, 139.5] loss: 0.018, acc: 0.039, f1: 0.038
[21, 386.1] loss: 0.018, acc: 0.062, f1: 0.043
[22, 139.2] loss: 0.017, acc: 0.035, f1: 0.013
[23, 139.7] loss: 0.017, acc: 0.059, f1: 0.181
[24, 140.7] loss: 0.017, acc: 0.080, f1: 0.208
[25, 138.7] loss: 0.016, acc: 0.067, f1: 0.219
[26, 140.4] loss: 0.016, acc: 0.100, f1: 0.254
[27, 141.0] loss: 0.016, acc: 0.096, f1: 0.249
[28, 140.5] loss: 0.015, acc: 0.100, f1: 0.261
[29, 140.3] loss: 0.015, acc: 0.112, f1: 0.251
[30, 140.9] loss: 0.015, acc: 0.081, f1: 0.224
[31, 139.6] loss: 0.014, acc: 0.125, f1: 0.262
[32, 139.1] loss: 0.014, acc: 0.128, f1: 0.207
[33, 139.9] loss: 0.014, acc: 0.133, f1: 0.304
[34, 139.7] loss: 0.014, acc: 0.152, f1: 0.331
[35, 140.6] loss: 0.013, acc: 0.135, f1: 0.311
[36, 141.6] loss: 0.013, acc: 0.160, f1: 0.353
[37, 142.0] loss: 0.013, acc: 0.141, f1: 0.316
[38, 141.6] loss: 0.013, acc: 0.183, f1: 0.380
[39, 139.3] loss: 0.013, acc: 0.195, f1: 0.377
[40, 139.8] loss: 0.012, acc: 0.191, f1: 0.401
[41, 385.7] loss: 0.012, acc: 0.192, f1: 0.399
[42, 140.1] loss: 0.012, acc: 0.214, f1: 0.420
[43, 140.4] loss: 0.012, acc: 0.233, f1: 0.437
[44, 140.0] loss: 0.011, acc: 0.219, f1: 0.285
[45, 140.8] loss: 0.011, acc: 0.220, f1: 0.431
[46, 140.9] loss: 0.011, acc: 0.235, f1: 0.426
[47, 141.0] loss: 0.011, acc: 0.242, f1: 0.443
[48, 139.2] loss: 0.011, acc: 0.252, f1: 0.427
[49, 141.5] loss: 0.010, acc: 0.281, f1: 0.437
[50, 139.5] loss: 0.010, acc: 0.257, f1: 0.417
[51, 140.0] loss: 0.010, acc: 0.271, f1: 0.459
[52, 139.6] loss: 0.010, acc: 0.288, f1: 0.490
[53, 140.6] loss: 0.010, acc: 0.292, f1: 0.485
[54, 140.1] loss: 0.010, acc: 0.299, f1: 0.499
[55, 139.9] loss: 0.009, acc: 0.269, f1: 0.457
[56, 140.9] loss: 0.009, acc: 0.336, f1: 0.498
[57, 142.3] loss: 0.009, acc: 0.310, f1: 0.504
[58, 140.6] loss: 0.009, acc: 0.315, f1: 0.514
[59, 140.1] loss: 0.009, acc: 0.333, f1: 0.523
[60, 140.7] loss: 0.008, acc: 0.344, f1: 0.534
[61, 385.4] loss: 0.008, acc: 0.336, f1: 0.545
[62, 139.0] loss: 0.008, acc: 0.341, f1: 0.541
[63, 139.3] loss: 0.008, acc: 0.335, f1: 0.542
[64, 139.3] loss: 0.008, acc: 0.348, f1: 0.547
[65, 142.1] loss: 0.008, acc: 0.333, f1: 0.540
[66, 140.6] loss: 0.007, acc: 0.303, f1: 0.478
[67, 140.3] loss: 0.007, acc: 0.366, f1: 0.561
[68, 140.0] loss: 0.007, acc: 0.379, f1: 0.570
[69, 141.4] loss: 0.007, acc: 0.357, f1: 0.554
[70, 140.2] loss: 0.007, acc: 0.373, f1: 0.564
[71, 139.3] loss: 0.007, acc: 0.358, f1: 0.547
[72, 139.3] loss: 0.007, acc: 0.388, f1: 0.568
[73, 140.3] loss: 0.007, acc: 0.392, f1: 0.579
[74, 139.8] loss: 0.007, acc: 0.388, f1: 0.578
[75, 139.9] loss: 0.006, acc: 0.390, f1: 0.561
[76, 140.5] loss: 0.006, acc: 0.405, f1: 0.583
[77, 139.6] loss: 0.006, acc: 0.402, f1: 0.583
[78, 139.4] loss: 0.006, acc: 0.404, f1: 0.583
[79, 141.0] loss: 0.006, acc: 0.411, f1: 0.581
[80, 140.3] loss: 0.006, acc: 0.400, f1: 0.586
[81, 387.7] loss: 0.006, acc: 0.395, f1: 0.583
[82, 140.0] loss: 0.006, acc: 0.414, f1: 0.590
[83, 140.0] loss: 0.006, acc: 0.402, f1: 0.586
[84, 139.9] loss: 0.006, acc: 0.401, f1: 0.578
[85, 139.1] loss: 0.006, acc: 0.432, f1: 0.612
[86, 139.5] loss: 0.005, acc: 0.444, f1: 0.604
[87, 139.8] loss: 0.005, acc: 0.423, f1: 0.606
[88, 139.9] loss: 0.005, acc: 0.461, f1: 0.624
[89, 140.3] loss: 0.005, acc: 0.417, f1: 0.596
[90, 139.8] loss: 0.005, acc: 0.411, f1: 0.583
[91, 141.0] loss: 0.005, acc: 0.436, f1: 0.603
[92, 141.8] loss: 0.005, acc: 0.430, f1: 0.593
[93, 140.1] loss: 0.005, acc: 0.442, f1: 0.608
[94, 139.9] loss: 0.005, acc: 0.402, f1: 0.582
[95, 140.2] loss: 0.005, acc: 0.436, f1: 0.598
[96, 140.1] loss: 0.005, acc: 0.437, f1: 0.606
[97, 141.0] loss: 0.005, acc: 0.410, f1: 0.559
[98, 141.1] loss: 0.005, acc: 0.444, f1: 0.598
[99, 140.2] loss: 0.005, acc: 0.448, f1: 0.615
[100, 141.9] loss: 0.004, acc: 0.452, f1: 0.610
[101, 388.0] loss: 0.004, acc: 0.458, f1: 0.607
[102, 138.8] loss: 0.004, acc: 0.438, f1: 0.610
[103, 139.4] loss: 0.004, acc: 0.467, f1: 0.605
[104, 139.8] loss: 0.004, acc: 0.473, f1: 0.626
[105, 139.3] loss: 0.004, acc: 0.460, f1: 0.608
[106, 139.5] loss: 0.004, acc: 0.461, f1: 0.610
[107, 139.9] loss: 0.004, acc: 0.448, f1: 0.613
[108, 139.5] loss: 0.004, acc: 0.474, f1: 0.630
[109, 139.2] loss: 0.004, acc: 0.482, f1: 0.640
[110, 138.3] loss: 0.004, acc: 0.482, f1: 0.642
[111, 138.7] loss: 0.004, acc: 0.434, f1: 0.599
[112, 139.1] loss: 0.004, acc: 0.457, f1: 0.620
[113, 138.8] loss: 0.004, acc: 0.455, f1: 0.612
[114, 139.4] loss: 0.004, acc: 0.477, f1: 0.634
[115, 139.7] loss: 0.004, acc: 0.495, f1: 0.633
[116, 141.0] loss: 0.003, acc: 0.489, f1: 0.626
[117, 140.8] loss: 0.004, acc: 0.447, f1: 0.603
[118, 139.4] loss: 0.004, acc: 0.452, f1: 0.617
[119, 139.6] loss: 0.004, acc: 0.466, f1: 0.612
[120, 140.1] loss: 0.003, acc: 0.501, f1: 0.632
[121, 384.8] loss: 0.003, acc: 0.477, f1: 0.628
[122, 139.1] loss: 0.003, acc: 0.474, f1: 0.626
[123, 139.9] loss: 0.004, acc: 0.487, f1: 0.623
[124, 140.1] loss: 0.003, acc: 0.483, f1: 0.623
[125, 141.3] loss: 0.003, acc: 0.455, f1: 0.600
[126, 140.1] loss: 0.004, acc: 0.452, f1: 0.614
[127, 140.3] loss: 0.003, acc: 0.452, f1: 0.612
[128, 140.7] loss: 0.005, acc: 0.464, f1: 0.624
[129, 141.1] loss: 0.003, acc: 0.473, f1: 0.623
[130, 140.7] loss: 0.003, acc: 0.486, f1: 0.628
[131, 140.4] loss: 0.003, acc: 0.477, f1: 0.627
[132, 140.4] loss: 0.003, acc: 0.470, f1: 0.622
[133, 139.9] loss: 0.003, acc: 0.456, f1: 0.610
[134, 140.1] loss: 0.003, acc: 0.458, f1: 0.617
[135, 140.4] loss: 0.003, acc: 0.462, f1: 0.616
[136, 140.6] loss: 0.003, acc: 0.517, f1: 0.643
[137, 140.7] loss: 0.003, acc: 0.502, f1: 0.639
[138, 140.0] loss: 0.003, acc: 0.436, f1: 0.599
[139, 139.3] loss: 0.003, acc: 0.489, f1: 0.640
[140, 139.6] loss: 0.003, acc: 0.492, f1: 0.635
[141, 387.4] loss: 0.003, acc: 0.504, f1: 0.634
[142, 140.2] loss: 0.003, acc: 0.441, f1: 0.597
[143, 140.7] loss: 0.003, acc: 0.457, f1: 0.612
[144, 139.7] loss: 0.003, acc: 0.486, f1: 0.619
[146, 140.1] loss: 0.003, acc: 0.473, f1: 0.613
[147, 139.9] loss: 0.003, acc: 0.508, f1: 0.650
[148, 141.0] loss: 0.003, acc: 0.494, f1: 0.633
[149, 139.7] loss: 0.003, acc: 0.465, f1: 0.610
[150, 140.1] loss: 0.003, acc: 0.485, f1: 0.631
[151, 140.5] loss: 0.003, acc: 0.482, f1: 0.615
[152, 140.1] loss: 0.003, acc: 0.498, f1: 0.634
[153, 140.1] loss: 0.002, acc: 0.475, f1: 0.617
[154, 140.1] loss: 0.002, acc: 0.482, f1: 0.622
[155, 141.0] loss: 0.002, acc: 0.498, f1: 0.633
[156, 139.4] loss: 0.002, acc: 0.487, f1: 0.639
[157, 141.4] loss: 0.003, acc: 0.477, f1: 0.628
[158, 142.5] loss: 0.002, acc: 0.463, f1: 0.606
[159, 140.7] loss: 0.002, acc: 0.454, f1: 0.603
[160, 140.0] loss: 0.002, acc: 0.487, f1: 0.626
[161, 388.0] loss: 0.002, acc: 0.490, f1: 0.621
[162, 140.2] loss: 0.002, acc: 0.473, f1: 0.601
[163, 140.1] loss: 0.002, acc: 0.479, f1: 0.616
[164, 139.8] loss: 0.002, acc: 0.477, f1: 0.617
[165, 140.7] loss: 0.002, acc: 0.467, f1: 0.613
[166, 140.0] loss: 0.003, acc: 0.490, f1: 0.620
[167, 139.9] loss: 0.002, acc: 0.492, f1: 0.634
[168, 140.9] loss: 0.002, acc: 0.505, f1: 0.634
[169, 140.4] loss: 0.002, acc: 0.481, f1: 0.608
[170, 139.6] loss: 0.002, acc: 0.497, f1: 0.635
[171, 140.3] loss: 0.002, acc: 0.477, f1: 0.612
[172, 140.5] loss: 0.002, acc: 0.481, f1: 0.618
[173, 140.8] loss: 0.002, acc: 0.503, f1: 0.636
[174, 139.3] loss: 0.002, acc: 0.487, f1: 0.618
[175, 140.5] loss: 0.002, acc: 0.498, f1: 0.631
[176, 139.9] loss: 0.002, acc: 0.470, f1: 0.604
[177, 139.5] loss: 0.002, acc: 0.506, f1: 0.629
[178, 140.7] loss: 0.002, acc: 0.486, f1: 0.623
[179, 141.0] loss: 0.002, acc: 0.520, f1: 0.643
[180, 141.2] loss: 0.002, acc: 0.496, f1: 0.625
[181, 386.7] loss: 0.002, acc: 0.528, f1: 0.646
[182, 140.1] loss: 0.002, acc: 0.500, f1: 0.624
[183, 139.9] loss: 0.002, acc: 0.478, f1: 0.608
[184, 139.7] loss: 0.002, acc: 0.492, f1: 0.619
[185, 139.9] loss: 0.002, acc: 0.505, f1: 0.635
[186, 141.0] loss: 0.002, acc: 0.492, f1: 0.616
[187, 141.5] loss: 0.002, acc: 0.461, f1: 0.614
[188, 140.7] loss: 0.002, acc: 0.516, f1: 0.630
[189, 140.1] loss: 0.002, acc: 0.502, f1: 0.626
[190, 139.5] loss: 0.002, acc: 0.502, f1: 0.615
[191, 140.0] loss: 0.002, acc: 0.523, f1: 0.646
[192, 140.2] loss: 0.002, acc: 0.488, f1: 0.610
[193, 140.5] loss: 0.002, acc: 0.504, f1: 0.639
[194, 141.1] loss: 0.002, acc: 0.492, f1: 0.629
[195, 140.6] loss: 0.002, acc: 0.492, f1: 0.612
[196, 141.6] loss: 0.002, acc: 0.524, f1: 0.652
[197, 141.5] loss: 0.002, acc: 0.536, f1: 0.652
[198, 140.1] loss: 0.002, acc: 0.508, f1: 0.635
[199, 140.4] loss: 0.002, acc: 0.533, f1: 0.649
[200, 140.6] loss: 0.002, acc: 0.520, f1: 0.627
[201, 374.3] loss: 0.001, acc: 0.511, f1: 0.626
[202, 139.8] loss: 0.002, acc: 0.516, f1: 0.634
[203, 141.0] loss: 0.001, acc: 0.520, f1: 0.643
[204, 141.3] loss: 0.002, acc: 0.521, f1: 0.612
[205, 140.2] loss: 0.002, acc: 0.515, f1: 0.643
[206, 140.1] loss: 0.001, acc: 0.491, f1: 0.621
[207, 140.6] loss: 0.001, acc: 0.486, f1: 0.607
[208, 140.3] loss: 0.002, acc: 0.528, f1: 0.636
[209, 141.2] loss: 0.002, acc: 0.472, f1: 0.603
[210, 140.0] loss: 0.002, acc: 0.508, f1: 0.642
[211, 139.9] loss: 0.002, acc: 0.453, f1: 0.596
[212, 140.4] loss: 0.001, acc: 0.525, f1: 0.648
[213, 142.0] loss: 0.002, acc: 0.502, f1: 0.633
[214, 140.4] loss: 0.002, acc: 0.495, f1: 0.619
[215, 139.1] loss: 0.002, acc: 0.507, f1: 0.641
[216, 140.3] loss: 0.002, acc: 0.479, f1: 0.621
[217, 139.6] loss: 0.002, acc: 0.523, f1: 0.642
[218, 139.3] loss: 0.002, acc: 0.467, f1: 0.602
[219, 140.3] loss: 0.001, acc: 0.508, f1: 0.627
[220, 141.3] loss: 0.001, acc: 0.533, f1: 0.653
[221, 385.7] loss: 0.001, acc: 0.476, f1: 0.617
[222, 139.4] loss: 0.002, acc: 0.490, f1: 0.616
[223, 140.4] loss: 0.001, acc: 0.496, f1: 0.620
[224, 138.8] loss: 0.001, acc: 0.525, f1: 0.645
[225, 139.6] loss: 0.002, acc: 0.511, f1: 0.628
[226, 140.2] loss: 0.001, acc: 0.510, f1: 0.639
[227, 140.5] loss: 0.002, acc: 0.486, f1: 0.609
[228, 139.2] loss: 0.002, acc: 0.509, f1: 0.637
[229, 140.3] loss: 0.001, acc: 0.508, f1: 0.634
[230, 139.8] loss: 0.001, acc: 0.520, f1: 0.644
[231, 139.7] loss: 0.001, acc: 0.520, f1: 0.645
[232, 140.0] loss: 0.001, acc: 0.515, f1: 0.637
[233, 139.9] loss: 0.001, acc: 0.530, f1: 0.648
[234, 139.7] loss: 0.001, acc: 0.533, f1: 0.649
[235, 140.5] loss: 0.001, acc: 0.506, f1: 0.631
[236, 139.8] loss: 0.001, acc: 0.511, f1: 0.637
[237, 139.7] loss: 0.001, acc: 0.505, f1: 0.648
[238, 139.9] loss: 0.002, acc: 0.503, f1: 0.624
[239, 139.9] loss: 0.001, acc: 0.522, f1: 0.630
[240, 140.3] loss: 0.002, acc: 0.528, f1: 0.646
[241, 389.0] loss: 0.001, acc: 0.527, f1: 0.646
[242, 139.6] loss: 0.001, acc: 0.505, f1: 0.605
[243, 139.4] loss: 0.001, acc: 0.541, f1: 0.643
[244, 139.9] loss: 0.001, acc: 0.519, f1: 0.635
[245, 139.3] loss: 0.001, acc: 0.498, f1: 0.614
[246, 139.8] loss: 0.001, acc: 0.472, f1: 0.588
[247, 139.6] loss: 0.001, acc: 0.507, f1: 0.628
[248, 140.3] loss: 0.001, acc: 0.520, f1: 0.637
[249, 140.3] loss: 0.001, acc: 0.467, f1: 0.611
[250, 140.1] loss: 0.001, acc: 0.484, f1: 0.616
[251, 139.7] loss: 0.001, acc: 0.510, f1: 0.637
[252, 141.5] loss: 0.001, acc: 0.511, f1: 0.621
[253, 140.1] loss: 0.001, acc: 0.505, f1: 0.616
[254, 140.8] loss: 0.001, acc: 0.530, f1: 0.643
[255, 140.2] loss: 0.001, acc: 0.538, f1: 0.646
[256, 140.4] loss: 0.001, acc: 0.513, f1: 0.628
[257, 141.8] loss: 0.002, acc: 0.522, f1: 0.621
[258, 139.9] loss: 0.001, acc: 0.542, f1: 0.648
[259, 141.6] loss: 0.001, acc: 0.515, f1: 0.632
[260, 140.6] loss: 0.001, acc: 0.530, f1: 0.627
[261, 387.3] loss: 0.001, acc: 0.542, f1: 0.647
[262, 140.6] loss: 0.001, acc: 0.525, f1: 0.644
[263, 140.4] loss: 0.001, acc: 0.555, f1: 0.659
[264, 141.5] loss: 0.001, acc: 0.511, f1: 0.621
[265, 140.4] loss: 0.001, acc: 0.514, f1: 0.608
[266, 141.7] loss: 0.001, acc: 0.514, f1: 0.626
[267, 141.1] loss: 0.001, acc: 0.523, f1: 0.632
[268, 141.7] loss: 0.001, acc: 0.492, f1: 0.602
[269, 140.1] loss: 0.001, acc: 0.511, f1: 0.614
[270, 140.9] loss: 0.001, acc: 0.520, f1: 0.622
[271, 141.4] loss: 0.001, acc: 0.535, f1: 0.639
[272, 140.6] loss: 0.001, acc: 0.481, f1: 0.603
[273, 140.2] loss: 0.001, acc: 0.539, f1: 0.646
[274, 140.1] loss: 0.001, acc: 0.542, f1: 0.637
[275, 140.5] loss: 0.001, acc: 0.527, f1: 0.622
[276, 140.6] loss: 0.001, acc: 0.533, f1: 0.642
[277, 141.8] loss: 0.001, acc: 0.480, f1: 0.581
[278, 141.0] loss: 0.001, acc: 0.521, f1: 0.627
[279, 139.9] loss: 0.001, acc: 0.502, f1: 0.603
[280, 140.9] loss: 0.001, acc: 0.527, f1: 0.622
[281, 384.4] loss: 0.001, acc: 0.558, f1: 0.638
[282, 141.1] loss: 0.001, acc: 0.561, f1: 0.646
[283, 141.1] loss: 0.001, acc: 0.519, f1: 0.631
[284, 140.0] loss: 0.001, acc: 0.523, f1: 0.636
[285, 141.3] loss: 0.001, acc: 0.517, f1: 0.616
[286, 141.5] loss: 0.001, acc: 0.525, f1: 0.623
[287, 141.8] loss: 0.001, acc: 0.529, f1: 0.619
[288, 141.2] loss: 0.001, acc: 0.555, f1: 0.651
[289, 140.4] loss: 0.001, acc: 0.538, f1: 0.640
[290, 141.0] loss: 0.001, acc: 0.511, f1: 0.617
[291, 140.4] loss: 0.001, acc: 0.521, f1: 0.631
[292, 139.6] loss: 0.001, acc: 0.530, f1: 0.639
[293, 140.5] loss: 0.001, acc: 0.532, f1: 0.634
[294, 140.6] loss: 0.001, acc: 0.513, f1: 0.627
[295, 140.4] loss: 0.001, acc: 0.519, f1: 0.634
[296, 140.6] loss: 0.001, acc: 0.543, f1: 0.647
[297, 140.6] loss: 0.001, acc: 0.517, f1: 0.622
[298, 140.5] loss: 0.001, acc: 0.501, f1: 0.610
[299, 142.3] loss: 0.001, acc: 0.508, f1: 0.604
[300, 141.6] loss: 0.001, acc: 0.538, f1: 0.635
[301, 387.7] loss: 0.001, acc: 0.506, f1: 0.614
[302, 141.7] loss: 0.001, acc: 0.545, f1: 0.643
[303, 159.9] loss: 0.001, acc: 0.545, f1: 0.643
[304, 146.1] loss: 0.001, acc: 0.538, f1: 0.646
[305, 141.2] loss: 0.001, acc: 0.501, f1: 0.599
[306, 142.6] loss: 0.001, acc: 0.502, f1: 0.602
[307, 141.9] loss: 0.001, acc: 0.539, f1: 0.642
[308, 141.9] loss: 0.001, acc: 0.546, f1: 0.658
[309, 141.7] loss: 0.001, acc: 0.522, f1: 0.642
[310, 141.4] loss: 0.001, acc: 0.541, f1: 0.652
[311, 183.5] loss: 0.001, acc: 0.533, f1: 0.642
[312, 179.5] loss: 0.001, acc: 0.532, f1: 0.627
[313, 140.7] loss: 0.001, acc: 0.534, f1: 0.636
[314, 140.9] loss: 0.001, acc: 0.535, f1: 0.638
[315, 141.5] loss: 0.001, acc: 0.535, f1: 0.628
[316, 142.7] loss: 0.001, acc: 0.513, f1: 0.617
[317, 142.1] loss: 0.001, acc: 0.499, f1: 0.624
[318, 142.1] loss: 0.001, acc: 0.523, f1: 0.610
[319, 141.3] loss: 0.001, acc: 0.533, f1: 0.625
[320, 140.7] loss: 0.001, acc: 0.502, f1: 0.602
[321, 385.7] loss: 0.001, acc: 0.463, f1: 0.577
[322, 140.9] loss: 0.001, acc: 0.536, f1: 0.626
[323, 140.8] loss: 0.001, acc: 0.526, f1: 0.626
[324, 142.4] loss: 0.001, acc: 0.536, f1: 0.632
[325, 140.2] loss: 0.001, acc: 0.536, f1: 0.619
[326, 140.5] loss: 0.001, acc: 0.498, f1: 0.597
[327, 141.0] loss: 0.001, acc: 0.534, f1: 0.613
[328, 140.2] loss: 0.001, acc: 0.538, f1: 0.626
[329, 140.2] loss: 0.001, acc: 0.544, f1: 0.643
[330, 143.2] loss: 0.001, acc: 0.520, f1: 0.621
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="s1">&#39;models/</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

