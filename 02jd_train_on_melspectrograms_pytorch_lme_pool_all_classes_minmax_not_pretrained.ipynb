{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "from birdcall.metrics import *\n",
    "from birdcall.ops import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16\n",
    "MAX_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_pickle('data/classes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_items = pd.read_pickle('data/all_train_items.pkl')\n",
    "\n",
    "# all_train_items_npy = []\n",
    "\n",
    "# for ebird_code, path, duration in all_train_items:\n",
    "#     fn = path.stem\n",
    "#     new_path = Path(f'data/npy/train_resampled/{ebird_code}/{fn}.npy')\n",
    "#     all_train_items_npy.append((ebird_code, new_path, duration))\n",
    "    \n",
    "# pd.to_pickle(all_train_items_npy, 'data/all_train_items_npy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pd.read_pickle('data/all_splits.pkl')\n",
    "all_train_items = pd.read_pickle('data/all_train_items_npy.pkl')\n",
    "\n",
    "train_items = np.array(all_train_items)[splits[0][0]].tolist()\n",
    "val_items = np.array(all_train_items)[splits[0][1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# class2train_items = defaultdict(list)\n",
    "\n",
    "# for cls_name, path, duration in train_items:\n",
    "#     class2train_items[cls_name].append((path, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(class2train_items, 'data/class2train_items.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2train_items = pd.read_pickle('data/class2train_items.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MelspecPoolDataset(class2train_items, classes, len_mult=50, normalize=False)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BS, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = [(classes.index(item[0]), item[1], item[2]) for item in val_items]\n",
    "val_items_binned = bin_items_negative_class(val_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(*list(torchvision.models.resnet34(False).children())[:-2])\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, len(classes))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        max_per_example = x.view(x.shape[0], -1).max(1)[0] # scaling to between 0 and 1\n",
    "        x /= max_per_example[:, None, None, None, None]     # per example!\n",
    "        bs, im_num = x.shape[:2]\n",
    "        x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = self.cnn(x)\n",
    "        x = x.mean((2,3))\n",
    "        x = self.classifier(x)\n",
    "        x = x.view(bs, im_num, -1)\n",
    "        x = lme_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), MAX_LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_items = pd.read_pickle('data/soundscape_items.pkl')\n",
    "\n",
    "# sc_items_npy = []\n",
    "# for labels, path, offset in sc_items:\n",
    "#     sc_items_npy.append((labels, Path(f'data/npy/shifted/{path.stem}.npy'), offset))\n",
    "    \n",
    "# pd.to_pickle(sc_items_npy, 'data/soundscape_items_npy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ds = SoundscapeMelspecPoolDataset(pd.read_pickle('data/soundscape_items_npy.pkl'), classes)\n",
    "sc_dl = torch.utils.data.DataLoader(sc_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14.7] loss: 0.023, f1: 0.000, sc_f1: 0.000\n",
      "[10, 29.3] loss: 0.020, f1: 0.000, sc_f1: 0.000\n",
      "[15, 44.0] loss: 0.018, f1: 0.039, sc_f1: 0.000\n",
      "[20, 58.6] loss: 0.017, f1: 0.122, sc_f1: 0.000\n",
      "[25, 73.3] loss: 0.015, f1: 0.243, sc_f1: 0.000\n",
      "[30, 87.9] loss: 0.014, f1: 0.327, sc_f1: 0.000\n",
      "[35, 102.6] loss: 0.012, f1: 0.401, sc_f1: 0.000\n",
      "[40, 117.3] loss: 0.011, f1: 0.492, sc_f1: 0.000\n",
      "[45, 131.9] loss: 0.010, f1: 0.547, sc_f1: 0.000\n",
      "[50, 146.6] loss: 0.009, f1: 0.552, sc_f1: 0.000\n",
      "[55, 161.2] loss: 0.008, f1: 0.603, sc_f1: 0.000\n",
      "[60, 175.9] loss: 0.007, f1: 0.620, sc_f1: 0.000\n",
      "[65, 190.6] loss: 0.006, f1: 0.643, sc_f1: 0.000\n",
      "[70, 205.2] loss: 0.006, f1: 0.664, sc_f1: 0.000\n",
      "[75, 219.8] loss: 0.005, f1: 0.676, sc_f1: 0.000\n",
      "[80, 234.5] loss: 0.005, f1: 0.681, sc_f1: 0.000\n",
      "[85, 249.1] loss: 0.004, f1: 0.677, sc_f1: 0.000\n",
      "[90, 263.8] loss: 0.004, f1: 0.682, sc_f1: 0.000\n",
      "[95, 278.4] loss: 0.003, f1: 0.700, sc_f1: 0.000\n",
      "[100, 293.1] loss: 0.003, f1: 0.702, sc_f1: 0.000\n",
      "[105, 307.7] loss: 0.003, f1: 0.701, sc_f1: 0.000\n",
      "[110, 322.4] loss: 0.003, f1: 0.704, sc_f1: 0.000\n",
      "[115, 337.0] loss: 0.002, f1: 0.703, sc_f1: 0.000\n",
      "[120, 351.7] loss: 0.002, f1: 0.696, sc_f1: 0.000\n",
      "[125, 366.3] loss: 0.002, f1: 0.703, sc_f1: 0.000\n",
      "[130, 381.0] loss: 0.002, f1: 0.710, sc_f1: 0.000\n",
      "[135, 395.6] loss: 0.002, f1: 0.715, sc_f1: 0.000\n",
      "[140, 410.3] loss: 0.002, f1: 0.705, sc_f1: 0.000\n",
      "[145, 424.9] loss: 0.002, f1: 0.707, sc_f1: 0.000\n",
      "[150, 439.6] loss: 0.002, f1: 0.706, sc_f1: 0.000\n",
      "[155, 454.2] loss: 0.002, f1: 0.707, sc_f1: 0.000\n",
      "[160, 468.9] loss: 0.001, f1: 0.705, sc_f1: 0.000\n",
      "[165, 483.5] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[170, 498.2] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[175, 512.8] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[180, 527.4] loss: 0.001, f1: 0.710, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(180):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if np.isnan(loss.item()): \n",
    "            raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_minmax_not_pretrained_{round(max(f1s), 2)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185, 14.6] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[190, 29.3] loss: 0.001, f1: 0.710, sc_f1: 0.000\n",
      "[195, 44.0] loss: 0.001, f1: 0.710, sc_f1: 0.000\n",
      "[200, 58.6] loss: 0.001, f1: 0.715, sc_f1: 0.000\n",
      "[205, 73.3] loss: 0.001, f1: 0.707, sc_f1: 0.000\n",
      "[210, 87.9] loss: 0.001, f1: 0.713, sc_f1: 0.000\n",
      "[215, 102.6] loss: 0.001, f1: 0.706, sc_f1: 0.000\n",
      "[220, 117.3] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[225, 132.0] loss: 0.001, f1: 0.708, sc_f1: 0.000\n",
      "[230, 146.7] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[235, 161.4] loss: 0.001, f1: 0.706, sc_f1: 0.000\n",
      "[240, 176.1] loss: 0.001, f1: 0.706, sc_f1: 0.000\n",
      "[245, 190.9] loss: 0.001, f1: 0.708, sc_f1: 0.000\n",
      "[250, 205.7] loss: 0.001, f1: 0.718, sc_f1: 0.000\n",
      "[255, 220.6] loss: 0.001, f1: 0.707, sc_f1: 0.000\n",
      "[260, 235.4] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[265, 250.3] loss: 0.001, f1: 0.700, sc_f1: 0.000\n",
      "[270, 265.2] loss: 0.001, f1: 0.709, sc_f1: 0.000\n",
      "[275, 280.0] loss: 0.001, f1: 0.712, sc_f1: 0.000\n",
      "[280, 294.8] loss: 0.001, f1: 0.709, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(180, 280):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if np.isnan(loss.item()): \n",
    "            raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        model.eval();\n",
    "        preds = []\n",
    "        targs = []\n",
    "\n",
    "        for num_specs in val_items_binned.keys():\n",
    "            valid_ds = MelspecShortishValidatioDataset(val_items_binned[num_specs], classes)\n",
    "            valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targs = torch.cat(targs)\n",
    "\n",
    "        f1s = []\n",
    "        ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "            ts.append(t)\n",
    "        \n",
    "        sc_preds = []\n",
    "        sc_targs = []\n",
    "        with torch.no_grad():\n",
    "            for data in sc_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)\n",
    "                sc_preds.append(outputs.cpu().detach())\n",
    "                sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "        sc_preds = torch.cat(sc_preds)\n",
    "        sc_targs = torch.cat(sc_targs)\n",
    "        sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "        \n",
    "        sc_f1s = []\n",
    "        sc_ts = []\n",
    "        for t in np.linspace(0.4, 1, 61):\n",
    "            sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "            sc_ts.append(t)\n",
    "        \n",
    "        print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}_lmepool_simple_minmax_not_pretrained_{round(max(f1s), 2)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models/280_lmepool_simple_minmax_not_pretrained_0.71.pth' target='_blank'>models/280_lmepool_simple_minmax_not_pretrained_0.71.pth</a><br>"
      ],
      "text/plain": [
       "/home/radek/workspace/birdcall/models/280_lmepool_simple_minmax_not_pretrained_0.71.pth"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('models/280_lmepool_simple_minmax_not_pretrained_0.71.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
