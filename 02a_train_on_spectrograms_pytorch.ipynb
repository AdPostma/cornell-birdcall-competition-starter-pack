{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "from birdcall.metrics import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_pickle('data/classes.pkl')\n",
    "train_ds = SpectrogramDataset(pd.read_pickle('data/train_set.pkl'), classes, len_mult=100, spec_max=80, spec_min=-100)\n",
    "valid_ds = SpectrogramDataset(pd.read_pickle('data/val_set.pkl'), classes, len_mult=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26400, 5280)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=120, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=2*120, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 3, 90, 714]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in train_dl: break\n",
    "b[0].shape, b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0143), tensor(0.9837))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].mean(), b[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50 = torchvision.models.resnet50(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom = nn.Sequential(*list(res50.children())[:6])\n",
    "mid = nn.Sequential(*list(res50.children())[6:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "                nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "                nn.Linear(1024, len(classes))\n",
    "            ]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "#         set_trace()\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(bottom, mid, Head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 264])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(b[0].cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 3, 90, 714])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 91.0] loss: 5.608, accuracy: 0.004\n",
      "[2, 90.3] loss: 5.421, accuracy: 0.005\n",
      "[3, 90.5] loss: 5.072, accuracy: 0.006\n",
      "[4, 90.4] loss: 4.833, accuracy: 0.006\n",
      "[6, 90.4] loss: 4.460, accuracy: 0.010\n",
      "[7, 90.3] loss: 4.306, accuracy: 0.007\n",
      "[8, 90.5] loss: 4.197, accuracy: 0.008\n",
      "[9, 90.4] loss: 4.066, accuracy: 0.008\n",
      "[10, 90.3] loss: 3.915, accuracy: 0.007\n",
      "[13, 90.4] loss: 3.597, accuracy: 0.005\n",
      "[14, 90.3] loss: 3.482, accuracy: 0.008\n",
      "[15, 90.5] loss: 3.377, accuracy: 0.007\n",
      "[16, 90.3] loss: 3.291, accuracy: 0.004\n",
      "[17, 90.6] loss: 3.190, accuracy: 0.005\n",
      "[18, 90.5] loss: 3.138, accuracy: 0.005\n",
      "[19, 90.1] loss: 2.967, accuracy: 0.006\n",
      "[20, 90.3] loss: 2.908, accuracy: 0.004\n",
      "[21, 90.5] loss: 2.851, accuracy: 0.006\n",
      "[22, 90.3] loss: 2.765, accuracy: 0.007\n",
      "[23, 90.4] loss: 2.676, accuracy: 0.004\n",
      "[24, 90.3] loss: 2.614, accuracy: 0.006\n",
      "[25, 90.4] loss: 2.548, accuracy: 0.004\n",
      "[26, 90.0] loss: 2.478, accuracy: 0.005\n",
      "[27, 90.5] loss: 2.425, accuracy: 0.003\n",
      "[28, 90.3] loss: 2.310, accuracy: 0.004\n",
      "[29, 90.7] loss: 2.295, accuracy: 0.005\n",
      "[30, 90.2] loss: 2.215, accuracy: 0.004\n",
      "[31, 90.2] loss: 2.140, accuracy: 0.005\n",
      "[32, 90.3] loss: 2.137, accuracy: 0.004\n",
      "[33, 90.4] loss: 2.061, accuracy: 0.004\n",
      "[34, 90.1] loss: 2.026, accuracy: 0.007\n",
      "[35, 90.2] loss: 1.981, accuracy: 0.004\n",
      "[36, 90.3] loss: 1.913, accuracy: 0.003\n",
      "[37, 90.4] loss: 1.876, accuracy: 0.004\n",
      "[38, 90.6] loss: 1.800, accuracy: 0.004\n",
      "[39, 90.5] loss: 1.783, accuracy: 0.002\n",
      "[40, 90.4] loss: 1.750, accuracy: 0.003\n",
      "[41, 90.4] loss: 1.706, accuracy: 0.003\n",
      "[42, 90.5] loss: 1.673, accuracy: 0.002\n",
      "[43, 90.5] loss: 1.601, accuracy: 0.003\n",
      "[44, 90.4] loss: 1.586, accuracy: 0.005\n",
      "[45, 90.5] loss: 1.566, accuracy: 0.004\n",
      "[46, 90.5] loss: 1.527, accuracy: 0.003\n",
      "[47, 90.4] loss: 1.512, accuracy: 0.003\n",
      "[48, 90.4] loss: 1.476, accuracy: 0.005\n",
      "[49, 90.5] loss: 1.470, accuracy: 0.004\n",
      "[50, 90.4] loss: 1.396, accuracy: 0.003\n",
      "[51, 90.7] loss: 1.362, accuracy: 0.003\n",
      "[52, 90.4] loss: 1.337, accuracy: 0.003\n",
      "[53, 90.3] loss: 1.312, accuracy: 0.004\n",
      "[54, 90.2] loss: 1.305, accuracy: 0.004\n",
      "[55, 90.4] loss: 1.287, accuracy: 0.005\n",
      "[56, 90.5] loss: 1.248, accuracy: 0.004\n",
      "[57, 90.3] loss: 1.221, accuracy: 0.004\n",
      "[58, 90.4] loss: 1.205, accuracy: 0.002\n",
      "[59, 90.4] loss: 1.193, accuracy: 0.004\n",
      "[60, 90.5] loss: 1.151, accuracy: 0.005\n",
      "[61, 90.5] loss: 1.130, accuracy: 0.002\n",
      "[62, 90.5] loss: 1.113, accuracy: 0.003\n",
      "[63, 90.2] loss: 1.068, accuracy: 0.004\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.argmax(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if i % len(train_dl) == len(train_dl)-1:\n",
    "            model.eval();\n",
    "            preds = []\n",
    "            targs = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in valid_dl:\n",
    "                    inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.cpu().detach())\n",
    "                    targs.append(labels.cpu().detach())\n",
    "\n",
    "                preds = torch.cat(preds)\n",
    "                targs = torch.cat(targs)\n",
    "            \n",
    "            accuracy = (targs.argmax(1) == preds.softmax(-1).argmax(1)).float().mean().item()\n",
    "            print(f'[{epoch + 1}, {time.time() - t0:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, accuracy: {accuracy:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to train with sigmoid activations and the BCE loss, but to no avail. Subsequently, I positioned the task in a way that should be eaiser for the model to train on (softmax, cross entropy loss), but again no go, same results.\n",
    "\n",
    "At this point I am becoming convinced that the model fails to generalize to unseen data. It could also explain some of the results I am seeing on kaggle - the disparity between the scores people get locally (with probably not the greatest way of sampling the validation set) vs their performance when they submit to the LB.\n",
    "\n",
    "To undertand this better, one additional datapoint I could collect is my model's performance on the train set. It achieves a low loss but what are its accuracy and f1?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
