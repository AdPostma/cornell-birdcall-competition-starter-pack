{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdcall.data import *\n",
    "from birdcall.metrics import *\n",
    "from birdcall.ops import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 100\n",
    "MAX_LR = 1e-3\n",
    "T_MAX = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_pickle('data/classes.pkl')\n",
    "splits = pd.read_pickle('data/all_splits.pkl')\n",
    "all_train_items = pd.read_pickle('data/all_train_items_npy.pkl')\n",
    "\n",
    "train_items = np.array(all_train_items)[splits[0][0]].tolist()\n",
    "val_items = np.array(all_train_items)[splits[0][1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17099, 4275)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_items), len(val_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data\n",
    "\n",
    "class MelspecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, items, classes):\n",
    "        self.items = items\n",
    "        self.vocab = classes\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        cls, path, _ = self.items[idx]\n",
    "        example = self.get_spec(path)\n",
    "        return example, self.one_hot_encode(cls)\n",
    "    \n",
    "    def get_spec(self, path):\n",
    "        frames_per_spec = 212  \n",
    "        x = np.load(path)\n",
    "\n",
    "        specs = []\n",
    "        for _ in range(3):\n",
    "            if x.shape[1] < frames_per_spec:\n",
    "                spec = np.zeros((80, frames_per_spec))\n",
    "                start_frame = np.random.randint(frames_per_spec-x.shape[1])\n",
    "                spec[:, start_frame:start_frame+x.shape[1]] = x\n",
    "            else:\n",
    "                start_frame = int(np.random.rand() * (x.shape[1] - frames_per_spec))\n",
    "                spec = x[:, start_frame:start_frame+frames_per_spec]\n",
    "            specs.append(spec)\n",
    "\n",
    "        return np.stack(specs).reshape(3, 80, frames_per_spec).astype(np.float32)\n",
    "    \n",
    "    def show(self, idx):\n",
    "        x = self[idx][0]\n",
    "        return plt.imshow(x.transpose(1,2,0)[:, :, 0])\n",
    "        \n",
    "    def one_hot_encode(self, cls):\n",
    "        y = self.vocab.index(cls)\n",
    "        one_hot = np.zeros((len(self.vocab)))\n",
    "        one_hot[y] = 1\n",
    "        return one_hot\n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MelspecDataset(train_items, classes)\n",
    "val_ds = MelspecDataset(val_items, classes)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BS, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(val_ds, batch_size=BS, num_workers=NUM_WORKERS, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.cnn = nn.Sequential(*list(torchvision.models.resnet50(True).children())[:-2], nn.AdaptiveMaxPool2d(1))\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, len(classes))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         max_per_example = x.view(x.shape[0], -1).max(1)[0]\n",
    "#         x /= max_per_example[:, None, None, None]\n",
    "        x = self.bn(x)\n",
    "        x = self.cnn(x)[:, :, 0, 0]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), MAX_LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ds = SoundscapeMelspecPoolDataset(pd.read_pickle('data/soundscape_items_npy.pkl'), classes)\n",
    "sc_dl = torch.utils.data.DataLoader(sc_ds, batch_size=2*BS, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.7] loss: 0.367, f1: 0.000, sc_f1: 0.000\n",
      "[2, 3.4] loss: 0.035, f1: 0.000, sc_f1: 0.000\n",
      "[3, 5.1] loss: 0.027, f1: 0.000, sc_f1: 0.000\n",
      "[4, 6.8] loss: 0.026, f1: 0.001, sc_f1: 0.000\n",
      "[5, 8.6] loss: 0.026, f1: 0.000, sc_f1: 0.000\n",
      "[6, 10.3] loss: 0.026, f1: 0.000, sc_f1: 0.000\n",
      "[7, 12.0] loss: 0.026, f1: 0.000, sc_f1: 0.000\n",
      "[8, 13.7] loss: 0.025, f1: 0.001, sc_f1: 0.000\n",
      "[9, 15.4] loss: 0.025, f1: 0.000, sc_f1: 0.000\n",
      "[10, 17.1] loss: 0.025, f1: 0.000, sc_f1: 0.000\n",
      "[11, 18.8] loss: 0.024, f1: 0.000, sc_f1: 0.000\n",
      "[12, 20.5] loss: 0.024, f1: 0.000, sc_f1: 0.000\n",
      "[13, 22.2] loss: 0.024, f1: 0.000, sc_f1: 0.000\n",
      "[14, 23.9] loss: 0.024, f1: 0.000, sc_f1: 0.000\n",
      "[15, 25.6] loss: 0.024, f1: 0.002, sc_f1: 0.000\n",
      "[16, 27.4] loss: 0.023, f1: 0.000, sc_f1: 0.000\n",
      "[17, 29.1] loss: 0.023, f1: 0.000, sc_f1: 0.000\n",
      "[18, 30.8] loss: 0.023, f1: 0.001, sc_f1: 0.000\n",
      "[19, 32.5] loss: 0.023, f1: 0.000, sc_f1: 0.000\n",
      "[20, 34.2] loss: 0.023, f1: 0.001, sc_f1: 0.000\n",
      "[21, 35.9] loss: 0.022, f1: 0.002, sc_f1: 0.000\n",
      "[22, 37.6] loss: 0.022, f1: 0.002, sc_f1: 0.000\n",
      "[23, 39.3] loss: 0.022, f1: 0.002, sc_f1: 0.000\n",
      "[24, 41.0] loss: 0.022, f1: 0.004, sc_f1: 0.000\n",
      "[25, 42.7] loss: 0.022, f1: 0.003, sc_f1: 0.000\n",
      "[26, 44.5] loss: 0.022, f1: 0.003, sc_f1: 0.000\n",
      "[27, 46.2] loss: 0.022, f1: 0.006, sc_f1: 0.000\n",
      "[28, 47.9] loss: 0.021, f1: 0.019, sc_f1: 0.000\n",
      "[29, 49.6] loss: 0.021, f1: 0.001, sc_f1: 0.000\n",
      "[30, 51.3] loss: 0.021, f1: 0.017, sc_f1: 0.000\n",
      "[31, 53.0] loss: 0.021, f1: 0.014, sc_f1: 0.000\n",
      "[32, 54.7] loss: 0.021, f1: 0.023, sc_f1: 0.000\n",
      "[33, 56.4] loss: 0.020, f1: 0.030, sc_f1: 0.000\n",
      "[34, 58.1] loss: 0.020, f1: 0.020, sc_f1: 0.000\n",
      "[35, 59.8] loss: 0.020, f1: 0.021, sc_f1: 0.000\n",
      "[36, 61.5] loss: 0.020, f1: 0.040, sc_f1: 0.000\n",
      "[37, 63.2] loss: 0.020, f1: 0.008, sc_f1: 0.000\n",
      "[38, 65.0] loss: 0.020, f1: 0.042, sc_f1: 0.000\n",
      "[39, 66.7] loss: 0.020, f1: 0.077, sc_f1: 0.000\n",
      "[40, 68.4] loss: 0.019, f1: 0.059, sc_f1: 0.000\n",
      "[41, 70.1] loss: 0.019, f1: 0.088, sc_f1: 0.000\n",
      "[42, 71.8] loss: 0.018, f1: 0.092, sc_f1: 0.000\n",
      "[43, 73.5] loss: 0.018, f1: 0.082, sc_f1: 0.000\n",
      "[44, 75.2] loss: 0.018, f1: 0.096, sc_f1: 0.000\n",
      "[45, 76.9] loss: 0.018, f1: 0.113, sc_f1: 0.000\n",
      "[46, 78.6] loss: 0.018, f1: 0.145, sc_f1: 0.000\n",
      "[47, 80.3] loss: 0.018, f1: 0.109, sc_f1: 0.000\n",
      "[48, 82.0] loss: 0.018, f1: 0.125, sc_f1: 0.000\n",
      "[49, 83.8] loss: 0.017, f1: 0.182, sc_f1: 0.000\n",
      "[50, 85.5] loss: 0.017, f1: 0.199, sc_f1: 0.000\n",
      "[51, 87.2] loss: 0.017, f1: 0.191, sc_f1: 0.000\n",
      "[52, 88.9] loss: 0.016, f1: 0.230, sc_f1: 0.000\n",
      "[53, 90.6] loss: 0.016, f1: 0.229, sc_f1: 0.000\n",
      "[54, 92.3] loss: 0.016, f1: 0.201, sc_f1: 0.000\n",
      "[55, 94.0] loss: 0.016, f1: 0.187, sc_f1: 0.000\n",
      "[56, 95.7] loss: 0.016, f1: 0.234, sc_f1: 0.000\n",
      "[57, 97.4] loss: 0.015, f1: 0.201, sc_f1: 0.000\n",
      "[58, 99.1] loss: 0.015, f1: 0.275, sc_f1: 0.000\n",
      "[59, 100.8] loss: 0.015, f1: 0.265, sc_f1: 0.000\n",
      "[60, 102.5] loss: 0.015, f1: 0.173, sc_f1: 0.000\n",
      "[61, 104.2] loss: 0.014, f1: 0.297, sc_f1: 0.000\n",
      "[62, 105.9] loss: 0.014, f1: 0.275, sc_f1: 0.000\n",
      "[63, 107.6] loss: 0.014, f1: 0.210, sc_f1: 0.000\n",
      "[64, 109.4] loss: 0.014, f1: 0.270, sc_f1: 0.000\n",
      "[65, 111.1] loss: 0.014, f1: 0.282, sc_f1: 0.000\n",
      "[66, 112.8] loss: 0.014, f1: 0.272, sc_f1: 0.000\n",
      "[67, 114.5] loss: 0.014, f1: 0.190, sc_f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        model.train()\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if np.isnan(loss.item()): \n",
    "            raise Exception(f'!!! nan encountered in loss !!! epoch: {epoch}\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval();\n",
    "    preds = []\n",
    "    targs = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            for data in valid_dl:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(inputs)\n",
    "                preds.append(outputs.cpu().detach())\n",
    "                targs.append(labels.cpu().detach())\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    targs = torch.cat(targs)\n",
    "\n",
    "    f1s = []\n",
    "    ts = []\n",
    "    for t in np.linspace(0.4, 1, 61):\n",
    "        f1s.append(f1_score(preds.sigmoid() > t, targs, average='micro'))\n",
    "        ts.append(t)\n",
    "\n",
    "    sc_preds = []\n",
    "    sc_targs = []\n",
    "    with torch.no_grad():\n",
    "        for data in sc_dl:\n",
    "            inputs, labels = data[0].squeeze(1).cuda(), data[1].cuda()\n",
    "            outputs = model(inputs)\n",
    "            sc_preds.append(outputs.cpu().detach())\n",
    "            sc_targs.append(labels.cpu().detach())\n",
    "\n",
    "    sc_preds = torch.cat(sc_preds)\n",
    "    sc_targs = torch.cat(sc_targs)\n",
    "    sc_f1 = f1_score(sc_preds.sigmoid() > 0.5, sc_targs, average='micro')\n",
    "\n",
    "    sc_f1s = []\n",
    "    sc_ts = []\n",
    "    for t in np.linspace(0.4, 1, 61):\n",
    "        sc_f1s.append(f1_score(sc_preds.sigmoid() > t, sc_targs, average='micro'))\n",
    "        sc_ts.append(t)\n",
    "\n",
    "    f1 = f1_score(preds.sigmoid() > 0.5, targs, average='micro')\n",
    "    print(f'[{epoch + 1}, {(time.time() - t0)/60:.1f}] loss: {running_loss / (len(train_dl)-1):.3f}, f1: {max(f1s):.3f}, sc_f1: {max(sc_f1s):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/{epoch+1}_single_example_per_epoch_{round(f1, 2)}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
